{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer, LongT5Model\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "a72psVUwLJyy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1310.4546\n",
        "create character based embeddings using skip-gram algorithms\n",
        "https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/\n"
      ],
      "metadata": {
        "id": "3kUUaw8_z8v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1301.3781"
      ],
      "metadata": {
        "id": "IW46NN9N6gYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I took a walk in the park yesterday and enjoyed the sunny weather.\",\n",
        "    \"She made a delicious chocolate cake for her sister's birthday.\",\n",
        "    \"The train arrived on time, and I was able to get to work without any delays.\",\n",
        "    \"My friends and I are planning a weekend trip to the beach next month.\",\n",
        "    \"He found a new job in the city and is excited to start next week.\",\n",
        "    \"The movie we watched last night was really entertaining and full of action.\",\n",
        "    \"I bought a new book that I'm looking forward to reading over the weekend.\",\n",
        "    \"The restaurant we went to for dinner had the best pasta I've ever tasted.\",\n",
        "    \"She spent the afternoon organizing her closet and donating old clothes.\",\n",
        "    \"We decided to stay in and watch a movie because it was raining outside.\"\n",
        "]\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "characters = []\n",
        "char_pairs = []\n",
        "\n",
        "for sentence in dataset:\n",
        "    sentence = sentence.lower()\n",
        "    sentence_len = len(sentence)\n",
        "\n",
        "    for i, char in enumerate(sentence):\n",
        "        if char not in characters:\n",
        "            characters.append(char)\n",
        "\n",
        "        for j in range(-window_size, window_size + 1):\n",
        "            if j == 0:\n",
        "                continue\n",
        "            context_position = i + j\n",
        "            if context_position >= 0 and context_position < sentence_len:\n",
        "                context_char = sentence[context_position]\n",
        "                char_pairs.append((char, context_char))\n",
        "\n",
        "characters = sorted(characters)\n",
        "char_to_index = {char: index for index, char in enumerate(characters)}\n",
        "index_to_char = {index: char for index, char in enumerate(characters)}"
      ],
      "metadata": {
        "id": "ZteSQxvV2TtV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dims=100):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_dims)\n",
        "    self.output_layer = nn.Linear(self.embedding_dims, self.vocab_size)\n",
        "\n",
        "  def forward(self, input_word):\n",
        "    x = self.embedding_layer(input_word)\n",
        "    x = self.output_layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "tpZSbN81FYpK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, char_pairs, vocab_size, char_to_index):\n",
        "        self.char_pairs = char_pairs\n",
        "        self.vocab_size = vocab_size\n",
        "        self.char_to_index = char_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.char_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_word, target_word = self.char_pairs[idx]\n",
        "        input_word = self.char_to_index[input_word]\n",
        "        target_word = self.char_to_index[target_word]\n",
        "        return torch.tensor(input_word, dtype=torch.long), torch.tensor(target_word, dtype=torch.long)\n",
        "\n",
        "vocab_size = 29\n",
        "\n",
        "dataset = SkipGramDataset(char_pairs, vocab_size, char_to_index)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "sI61KL58DjxV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "model = SkipGram(vocab_size=29, embedding_dims=100)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for input_word, target_word in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_word)\n",
        "        loss = criterion(output, target_word)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R1dDjcqD42Q",
        "outputId": "63ca0ba6-f4df-4fdd-c29d-5d573d1be002"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.149599552154541\n",
            "Epoch 2, Loss: 3.0718226432800293\n",
            "Epoch 3, Loss: 3.427504301071167\n",
            "Epoch 4, Loss: 4.05959415435791\n",
            "Epoch 5, Loss: 2.4437053203582764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.embedding_layer.weight.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOzE-HKa-uUe",
        "outputId": "76a5e1b2-50de-4f36-a568-3a870f2479ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3451, -0.3230,  0.1157,  ..., -0.9892,  1.4008,  1.7541],\n",
              "        [ 0.5750,  0.8476, -1.1858,  ...,  0.7309, -0.0509, -1.2395],\n",
              "        [ 0.0851,  1.1289,  1.5185,  ..., -0.8972,  1.3207, -0.2341],\n",
              "        ...,\n",
              "        [ 1.7381,  1.7953,  0.6204,  ..., -0.6026, -1.7992,  1.1016],\n",
              "        [ 1.1075, -1.3388,  0.9358,  ..., -0.4265,  0.7825, -0.3104],\n",
              "        [ 0.3963,  0.5456,  1.4679,  ...,  0.9905, -1.3526,  0.8779]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}